<!DOCTYPE html>

<html>
<style>

</style>
<head>
    <title>CS1102 Group Project</title>    
</head>
<body>
    <h2>CS1102 - Course Project - 2023/2024 Semester B</h2>
    <h3>Group Members:</h3>
     <ul>
         <li>FOK Tsz Ying</li>
         <li>SUN Ying Kwing</li>
         <li>ZHANG Zhen</li>
         <li>CHAU Yin Tau</li>
     </ul><hr>
    <h1>Discovery</h1>

     <h2>1. Mechanism</h2>
     <p>In this project, we informed ourselves of the detailed mechanism of LLMs, which we normally would not have the chance of knowing. Namely, we learnt about the methodologies and technologies responsible for LLMs’ competencies, including deep learning, deep neural network and natural language processing.</p>

     <h2>2. Wide Range of Applications</h2>
     <p>As university students, our involvement with LLMs does not usually go beyond using ChatGPT to help with our studies. Therefore, it was eye-opening to learn about LLMs’ potential applications in diverse professional fields, such as education, healthcare, finance and the legal industry etc.</p>

     <h2>3. Frequent Hallucinations</h2>
     <p>It came as a shock to us that LLMs regularly give out deceiving or erroneous answers known as ‘hallucinations’ and even present them in an assertive and convincing tone. Therefore, it is important to check the validity and truthfulness of LLMs’ outputs to avoid being misled.</p>

     <h2>4. High Energy Usage</h2>
     <p>As users, we only have to type in prompts and enjoy quick and detailed answer provided by LLMs. Therefore, it was surprising to discover the high energy demand behind their trainings and the corresponding environmental issues.</p><hr>

    <h1>Summary</h1>

    <h2>1. Definition</h2>
    <p>A large language model (LLM) is an <b>artificial intelligence (AI) language model</b> that can learn to <b>understand and interpret human languages</b> via training with extensive amounts of data with the use of deep learning. In doing so, LLMs are ultimately capable of <b>complex language processing</b>, such as text generation, translation and sentiment analysis etc.</p>

    <h2>2. Architecture</h2>
    <p>LLMs gain their abilities through <b>deep learning</b>, which involves the employment of a multi-layered <b>deep neural network</b>. Inside the network, interconnected nodes within the <b>hidden layers</b> help LLMs perform analysis, computation or transformation on input data. Meanwhile, <b>Natural Language Processing (NLP)</b> is responsible for the understanding of human-language inputs and the generation of human-readable outputs.</p>
    <h4>Figure 1</h4>
    <p>Example of a deep neural network</p>
    <img src="network.png" alt="Example of a deep neural network" width="500px"></br>
    <p>Note. From File:Example of a deep neural network.png, by BrunelloN, 2021, Wikimedia Commons (<a href="https://commons.wikimedia.org/wiki/File:Example_of_a_deep_neural_network.png" target="_blank">https://commons.wikimedia.org/wiki/File:Example_of_a_deep_neural_network.png</a>).</p>

    <h2>3. Advantages of LLMs</h2>
    <ol>
        <li>Cross-domain Versatility: Easily applied across domains and useful for personalized content recommendations and summarizations</li><br>
        <li>Enhanced Security Applications: Identification of cybercrimes and protection of digital identities via interpretation and analysis of the visual data</li><br>
        <li>Multilingual Proficiency:</li>
          <ul>
             <li>Assist cross-cultural exchanges by overcoming language differences</li><br>
             <li>Facilitate democratization of information</li><br>
             <li>Globalization of businesses</li>
          </ul><br>
         <li>Revolutionizing Education through Personalization and Accessibility:</li>
          <ul>
             <li>Offer highly personalized learning experiences</li><br>
             <li>FCreate deeper engagements with interactive dialogues and feedback mechanisms</li><br>
             <li>Make education accessible regardless of geographical or financial barriers</li>
          </ul>
    </ol>

    <h2>4. Disadvantages of LLMs</h2>
    <ol>
        <li>Data Bias and Misinformation:  Reinforce Stereotypes & Perpetuate Inequalities</li><br>
        <li>Risk of Cognitive Complacency: Reliance on LLMs for problem-solving may hinder critical thinking and creativity</li><br>
        <li>Environmental Concerns: High energy consumption and carbon emissions from trainings of large LLMs</li><br>
        <li> Inaccurate/Ineffective Entity Matching</li>
    </ol>

<h2>5. Limitations of LLMs</h2>
    <ol>
        <li>Reliance on Training: processing data ncountered outside their training may often lead to inaccurate or illogical outputs</li><br>
        <li>Limited Input & Output Length</li>
    </ol><hr>

    <h1>Future of LLMs</h1>
    <h3>In the future, LLMs will continue to advance and become even better in text processing and generation. In addition, LLMs may be able to process visual and audio inputs for a better understanding of context so that they can be more effectively applied across different industries, such as:</h3>

    <h3>1. Healthcare</h3>
     <ul>
         <li><b>Allow patient communication unrestrained by time or location</b></li><br>
         <li><b>Optimize documentation process by organizing unstructured data</b></li><br>
         <li><b>Assist with public health initiatives via trend recognition and analysis</b></li>
     </ul>
    <p>(Clusmann et al., 2023)</p>

    <h3>2. Finance</h3>
     <ul>
         <li><b>Time Series Forecasting: Predict future price trends using historical data</b></li><br>
         <li><b>Portfolio Optimization: Support investment decisions across various assets</b></li><br>
         <li><b>Insolvency Forecasting: Analyze financial reports/ news sources to identify early symptoms of financial distress</b></li>
     </ul>
    <p>(Zhao et al., 2023)</p>

    <h3>3. Legal</h3>
     <ul>
         <li><b>Contract Review/ Drafting: Identify essential clauses, errors, potential risks etc.; draft brand new contracts that follow specific requirements/formats</b></li><br>
         <li><b>Litigation Analysis: Predict success rates, risks and costs to form effective strategies</b></li><br>
         <li><b>Legal research: Provide relevant information based on prompts given and answer questions based on findings</b></li>
     </ul>
    <p>(Sydorenko, 2023)</p>

    <h3>Furthermore, according to Toews (2024), here are three directions of development for LLMs that can mitigate their current flaws:</h3>

    <h3>1. Fine-tuning</h3>
    <p><b>Currently, LLMs rely on information consumed during their training to produce outputs. However, Toews points out that it could be possible that LLMs write new content themselves based on knowledge already obtained and use this content as new training data to ‘fine-tune’ itself. One may understandably question the logic of this approach as it seems like a useless cycle. However, a more accurate description would be doing self-reflection or self-questioning to gain new insights. Aside from self-improvement, LLMs being able to generate its own training data is also important as available text training data may soon be exhausted entirely.</b></p>

    <h3>2. Automated Fact-checking</h3>
    <p><b>Current efforts are being made to help improve the accuracy of LLMs’ outputs and avoid hallucinations. Specifically, developers wish to give LLMs the ability to gather information from external sources. This allows LLMs access to the most accurate and relevant information available online. Furthermore, developers hope to let LLMs provide references and citations for their answers. Not only does this increase validity, it also allows users to check if the sources provided are reliable.</b></p>

    <h3>3. Sparse Expert Models</h3>
    <p><b>Sparse expert models have a different architecture than many popular ‘dense’ LLMs nowadays. To elaborate, rather than activating all of its parameters everytime it is given a prompt, a sparse expert model only calls upon the ones that are the most relevant and helpful. Because of this, sparse expert models can be larger in scale but take less energy to train. Moreover, they outperform traditional LLMs while requiring much less computation. Even though sparse expert models have a more technically complex structure and the concept of it is not yet well-known, they hold massive potential for the future of LLMs.</b></p><hr>

    <h1>Reference List</h1>
    <ol>
        <li>Al-Hasan, T. M., Sayed, A. N., Bensaali, F., Himeur, Y., Varlamis, I., & Dimitrakopoulos, G. (2024). From Traditional Recommender Systems to GPT-Based Chatbots: A Survey of Recent Developments and Future Directions. <i>Big Data and Cognitive Computing, 8</i>(4). <a href="https://doi.org/10.3390/bdcc8040036" target="_blank">https://doi.org/10.3390/bdcc8040036</a></li><br>

        <li>Bak, M., & Chin, J. (2024). The potential and limitations of large language models in identification of the states of motivations for facilitating health behavior change. <i>Journal of the American Medical Informatics Association.</i> <a href="https://doi.org/10.1093/jamia/ocae057" target="_blank">https://doi.org/10.1093/jamia/ocae057</a></li><br>

        <li>BrunelloN. (2021, August 12). <i>File:Example_of_a_deep_neural_network.png.</i> Wikimedia Commons. <a href="https://commons.wikimedia.org/wiki/File:Example_of_a_deep_neural_network.png" target="_blank">https://commons.wikimedia.org/wiki/File:Example_of_a_deep_neural_network.png</a></li><br>

        <li>Cahyawijaya , S., Lovenia, H., & Fung, P. (2024). <i>LLMs Are Few-Shot In-Context Low-Resource Language Learners.</i> <a href="https://doi.org/10.48550/arXiv.2403.16512" target="_blank">https://doi.org/10.48550/arXiv.2403.16512</a></li><br>

        <li>Chaikiatsri, P., & Rattanasopon, S. (2024). <i>Evaluating the Multilingual Differences of ChatGPT and Google Gemini on the MMLU Dataset Translated into Thai.</i> <a href="https://doi.org/10.31219/osf.io/smkfu" target="_blank">https://doi.org/10.31219/osf.io/smkfu</a></li><br>

        <li>Clusmann, J., Kolbinger, F. R., Muti, H. S., Carrero, Z. I., Eckardt, J.-N., Laleh, N. G., Löffler, C. M. L., Schwarzkopf, S.-C., Unger, M., Veldhuizen, G. P., Wagner, S. J., & Kather, J. N. (2023, October 10). <i>The future landscape of large language models in medicine.</i> Nature News. <a href="https://www.nature.com/articles/s43856-023-00370-1" target="_blank">https://www.nature.com/articles/s43856-023-00370-1</a></li><br>

        <li>Daws, R. (2024, January 24). <i>NCSC: AI to significantly boost cyber threats over next two years.</i> AI News. <a href="https://www.artificialintelligence-news.com/2024/01/24/ncsc-ai-significantly-boost-cyber-threats-next-two-years/" target="_blank">https://www.artificialintelligence-news.com/2024/01/24/ncsc-ai-significantly-boost-cyber-threats-next-two-years/</a></li><br>

        <li>Ding, H., Dai, C., Wu, Y., Ma, W., & Zhou, H. (2024). Setem: Self-ensemble training with Pre-trained Language Models for Entity Matching. <i>Knowledge-Based Systems, 293.</i> <a href="https://doi.org/10.1016/j.knosys.2024.111708" target="_blank">https://doi.org/10.1016/j.knosys.2024.111708</a></li><br>

        <li>Fazackerley, A. (2023, March 19). <i>AI makes plagiarism harder to detect, argue academics – in paper written by chatbot.</i> The Guardian. <a href="https://www.theguardian.com/technology/2023/mar/19/ai-makes-plagiarism-harder-to-detect-argue-academics-in-paper-written-by-chatbot" target="_blank">https://www.theguardian.com/technology/2023/mar/19/ai-makes-plagiarism-harder-to-detect-argue-academics-in-paper-written-by-chatbot</a></li><br>

        <li><i>GPT-4 and GPT-4 Turbo.</i> OpenAI. (n.d.). <a href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo" target="_blank">https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo</a></li><br>

        <li>Gu, Z., Sun, X., Lian, F., Kang, Z., Xu, C., & Fan, J. (2024). Dingo: Towards Diverse and Fine-Grained Instruction-Following Evaluation. <i>Proceedings of the AAAI Conference on Artificial Intelligence, 38</i>(16), 18108–18116. <a href="https://doi.org/10.1609/aaai.v38i16.29768" target="_blank">https://doi.org/10.1609/aaai.v38i16.29768</a></li><br>

        <li><i>Hidden layer.</i> DeepAI. (2019, May 17). <a href="https://deepai.org/machine-learning-glossary-and-terms/hidden-layer-machine-learning" target="_blank">https://deepai.org/machine-learning-glossary-and-terms/hidden-layer-machine-learning</a></li><br>

        <li><i>Hidden layer.</i> DevX. (2023, December 19). <a href="https://www.devx.com/terms/hidden-layer/" target="_blank">https://www.devx.com/terms/hidden-layer/</a></li><br>

        <li><i>Introducing Llama: A foundational, 65-billion-parameter language model.</i> Meta. (2023, February 24). <a href=" https://ai.meta.com/blog/large-language-model-llama-meta-ai/" target="_blank"> https://ai.meta.com/blog/large-language-model-llama-meta-ai/</a></li><br>

        <li>Kasneci, E., Sessler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G., Günnemann, S., Hüllermeier, E., Krusche, S., Kutyniok, G., Michaeli, T., Nerdel, C., Pfeffer, J., Poquet, O., Sailer, M., Schmidt, A., Seidel, T., … Kasneci, G. (2023). Chatgpt for good? On opportunities and challenges of large language models for education. <i>Learning and Individual Differences, 103.</i> <a href="https://doi.org/10.1016/j.lindif.2023.102274" target="_blank">https://doi.org/10.1016/j.lindif.2023.102274</a></li><br>

        <li>Kerner, S. M. (2023, September 13). <i>What are large language models?: Definition from TechTarget.</i> WhatIs. <a href="https://www.techtarget.com/whatis/definition/large-language-model-LLM" target="_blank">https://www.techtarget.com/whatis/definition/large-language-model-LLM</a></li><br>

        <li>Kolbert, E. (2024, March 9). <i>The obscene energy demands of A.I.</i> The New Yorker. <a href="https://www.newyorker.com/news/daily-comment/the-obscene-energy-demands-of-ai" target="_blank">https://www.newyorker.com/news/daily-comment/the-obscene-energy-demands-of-ai</a></li><br>

        <li>Lewis, M., & Mitchell, M. (2024). <i>Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models.</i> <a href="https://doi.org/10.48550/arXiv.2402.08955" target="_blank">https://doi.org/10.48550/arXiv.2402.08955</a></li><br>

        <li>Marr, B. (2024, February 20). <i>A short history of ChatGPT: How we got to where we are today.</i> Forbes. <a href="https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-how-we-got-to-where-we-are-today/?sh=5df9e04b674f" target="_blank">https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-how-we-got-to-where-we-are-today/?sh=5df9e04b674f</a></li><br>

        <li><i>Natural Language Processing (NLP) - A complete guide.</i> DeepLearning.AI. (2023, January 11). <a href="https://www.deeplearning.ai/resources/natural-language-processing/" target="_blank">https://www.deeplearning.ai/resources/natural-language-processing/</a></li><br>

        <li>Nguyen, B. (2024, March 6). <i>ChatGPT is bad at following copyright law, researchers say.</i> Quartz. <a href="https://qz.com/openai-chatgpt-anthropic-claude-copyright-law-violation-1851311580" target="_blank">https://qz.com/openai-chatgpt-anthropic-claude-copyright-law-violation-1851311580</a></li><br>

        <li>Ognjanovski, G. (2020, June 7). <i>Everything you need to know about neural networks and backpropagation-machine learning made easy and fun.</i> Medium. <a href="https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a" target="_blank">https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a</a></li><br>

        <li>Park, K. (2023, May 2). <i>Samsung bans use of generative AI tools like ChatGPT after April internal data leak.</i> TechCrunch. <a href="https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/" target="_blank">https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/</a></li><br>

        <li>Pieuchon, N. A. de, Daoud, A., Jerzak, C. T., Johansson, M., & Johansson, R. (2024). <i>Can Large Language Models (or Humans) Distill Text?</i> <a href="https://doi.org/10.48550/arXiv.2403.16584" target="_blank">https://doi.org/10.48550/arXiv.2403.16584</a></li><br>

        <li>Proudfoot, O. (2024, March 22). <i>What is Claude AI, and how does it compare to ChatGPT?.</i> Pluralsight. <a href="https://www.pluralsight.com/resources/blog/data/what-is-claude-ai" target="_blank">https://www.pluralsight.com/resources/blog/data/what-is-claude-ai</a></li><br>

        <li>Sallam, M. (2023). <i>The Utility of ChatGPT as an Example of Large Language Models in Healthcare Education, Research and Practice: Systematic Review on the Future Perspectives and Potential Limitations.</i> <a href="https://doi.org/10.1101/2023.02.19.23286155" target="_blank">https://doi.org/10.1101/2023.02.19.23286155</a></li><br>

        <li>Shenwai, D. S. (2023, June 27). <i>8 potentially surprising things to know about large language models LLMs.</i> MarkTechPost. <a href="https://www.marktechpost.com/2023/06/27/8-potentially-surprising-things-to-know-about-large-language-models-llms/" target="_blank">https://www.marktechpost.com/2023/06/27/8-potentially-surprising-things-to-know-about-large-language-models-llms/</a></li><br>

        <li>Sydorenko, P. (2023, August 22). <i>Top 5 applications of large language models (LLMs) in legal practice.</i> Medium. <a href="https://medium.com/jurdep/top-5-applications-of-large-language-models-llms-in-legal-practice-d29cde9c38ef" target="_blank">https://medium.com/jurdep/top-5-applications-of-large-language-models-llms-in-legal-practice-d29cde9c38ef</a></li><br>

        <li>Toews, R. (2024, February 20). <i>The next generation of large language models.</i> Forbes. <a href="https://www.forbes.com/sites/robtoews/2023/02/07/the-next-generation-of-large-language-models/?sh=5b55b0af18db" target="_blank">https://www.forbes.com/sites/robtoews/2023/02/07/the-next-generation-of-large-language-models/?sh=5b55b0af18db</a></li><br>

        <li>Trad, F., & Chehab, A. (2024). <i>Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications.</i> <a href="https://doi.org/10.48550/arXiv.2403.17787" target="_blank">https://doi.org/10.48550/arXiv.2403.17787</a></li><br>

        <li><i>What are large language models (LLMs)?.</i> IBM. (n.d.-a). <a href="https://www.ibm.com/topics/large-language-models" target="_blank">https://www.ibm.com/topics/large-language-models</a></li><br>

        <li><i>What is deep learning?.</i> IBM. (n.d.-b). <a href="https://www.ibm.com/topics/deep-learning" target="_blank">https://www.ibm.com/topics/deep-learning</a></li><br>

        <li><i>What is the transformer architecture and how does it work?.</i> Datagen. (2023, May 22). <a href="https://datagen.tech/guides/computer-vision/transformer-architecture/" target="_blank">https://datagen.tech/guides/computer-vision/transformer-architecture/</a></li><br>

        <li>Zhao, H., Liu, Z., Wu, Z., Li, Y., Yang, T., Shu, P., Xu, S., Dai, H., Zhao, L., Mai, G., Liu, N., & Liu, T. (2024). <i>Revolutionizing Finance with LLMs: An Overview of Applications and Insights.</i> <a href="hhttps://doi.org/10.48550/arXiv.2401.11641" target="_blank">https://doi.org/10.48550/arXiv.2401.11641</a></li><br>

    </ol><hr>

    <h1>Go Back To...</h1>
    <h2><a href="index.html">Homepage</a></h2>
    <h2><a href="">Architecture</a></h2>
    <h2><a href="">Pros & Cons</a></h2>
    <h2><a href="">Limitations</a></h2>
</body>
</html>